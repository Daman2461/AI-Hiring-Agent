Synapse AI Hiring Agent - Hackathon Write-up

---

**Approach**

Our goal was to build an end-to-end AI-powered sourcing agent that could automate the discovery, scoring, and outreach to top LinkedIn candidates for any job description. We designed a modular system with a modern Streamlit UI, a FastAPI backend, and robust scraping and LLM-powered logic. The workflow starts with recruiter-style search queries generated by an LLM (Mistral) from the job description, which are then used to search Bing for LinkedIn profiles via Selenium. Candidate snippets are parsed, and each candidate is scored using a transparent rubric (education, trajectory, company, skills, location, tenure) by the LLM. Personalized outreach messages are also generated, highlighting each candidate's unique fit for the role. The system is extensible, with a clean API and a chat assistant for resume-based Q&A.

---

**Challenges Faced**

The biggest technical challenge was reliable candidate discovery at scale. Google blocks automated scraping aggressively, so we switched to Bing, which is more bot-friendly and still returns high-quality LinkedIn results. Parsing unstructured snippets into structured candidate data required careful prompt engineering and fallback logic. Ensuring the LLM returned valid, parseable JSON for scoring and messaging was another challenge, as was handling rate limits and occasional API failures. UI polish and user experience were also important, so we iterated on the Streamlit frontend for clarity and demo impact. Finally, we had to balance speed, robustness, and ethical considerations around scraping and automation.

---

**Scaling to 100s of Jobs**

To scale this system to hundreds of jobs, we would:
- **Batch and Parallelize:** Run multiple scraping and scoring jobs in parallel using async workers or a distributed task queue (e.g., Celery, RQ, or cloud functions).
- **Caching and Deduplication:** Cache search results and candidate profiles to avoid redundant scraping and API calls, and deduplicate candidates across similar jobs.
- **Multi-source Discovery:** Integrate additional sources (DuckDuckGo, direct LinkedIn search, job boards) to increase coverage and resilience.
- **Persistent Storage:** Store jobs, candidates, scores, and outreach messages in a database (e.g., PostgreSQL) for tracking, analytics, and re-use.
- **Robust Error Handling:** Implement retries, exponential backoff, and monitoring to handle scraping failures and API rate limits gracefully.
- **API-first Design:** Expose all core logic via FastAPI endpoints, making it easy to orchestrate, monitor, and scale from a central controller or workflow engine.
- **Cloud Deployment:** Deploy on scalable infrastructure (e.g., HuggingFace Spaces, AWS, GCP) to handle spikes in demand and enable team collaboration.

With these strategies, the system can efficiently process hundreds of jobs, discover thousands of candidates, and deliver high-quality, personalized outreach at scale. 