Synapse AI Hiring Agent - Hackathon Write-up

---

**Approach**

We set out to build a truly intelligent, end-to-end AI sourcing agent that automates the most time-consuming parts of technical recruiting. Our system combines a modern Streamlit UI, a robust FastAPI backend, and advanced browser automation (Selenium) to discover, score, and engage top LinkedIn candidates for any job. The workflow starts with a recruiter-style search query, generated by a state-of-the-art LLM (Mistral) from the job description. We use Selenium to scrape Bing for real LinkedIn profiles, bypassing Google's aggressive bot detection. Each candidate is parsed and scored using a transparent, multi-factor rubric (education, trajectory, company, skills, location, tenure), with the LLM providing nuanced, float-based scoring and a detailed breakdown. Personalized outreach messages are generated for every candidate, tailored to their unique profile and the job's requirements. The system is modular, API-first, and ready for both interactive and programmatic use.

---

**Challenges Faced**

The biggest technical challenge was robust, scalable candidate discovery. Google's anti-bot measures forced us to pivot to Bing, which, combined with Selenium, allowed us to reliably extract real LinkedIn profiles. Parsing unstructured snippets into structured candidate data required careful prompt engineering and fallback logic. Ensuring the LLM returned valid, parseable JSON for scoring and messaging was another challenge, as was handling rate limits and occasional API failures. We also had to design a UI that was both beautiful and functional, iterating on Streamlit layouts for clarity and demo impact. Finally, we balanced speed, robustness, and ethical considerations around scraping and automation, ensuring the system is both powerful and responsible.

---

**Scaling to 100s of Jobs**

To scale to hundreds of jobs, we would:
- **Batch and Parallelize:** Use async workers or distributed task queues (Celery, RQ, cloud functions) to run multiple scraping and scoring jobs in parallel.
- **Caching and Deduplication:** Cache search results and candidate profiles to avoid redundant scraping and API calls, and deduplicate candidates across similar jobs.
- **Multi-source Discovery:** Integrate additional sources (DuckDuckGo, direct LinkedIn search, job boards) for greater coverage and resilience.
- **Persistent Storage:** Store jobs, candidates, scores, and outreach messages in a database (PostgreSQL) for analytics and re-use.
- **Robust Error Handling:** Implement retries, exponential backoff, and monitoring to handle scraping failures and API rate limits gracefully.
- **Hybrid Architecture:** Deploy the UI on Streamlit Cloud or HuggingFace Spaces, with a Selenium-powered backend running on a dedicated VM or cloud server, enabling browser automation at scale.
- **API-first Design:** Expose all core logic via FastAPI endpoints for orchestration, monitoring, and integration with other systems.

With these strategies, our system can efficiently process hundreds of jobs, discover thousands of candidates, and deliver high-quality, personalized outreach at scaleâ€”empowering recruiters to focus on what matters most: building great teams. 